{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Plots\n",
    "\n",
    "The line plot is a fairly common plot type that is used to plot the trend of one numeric variable against values of a second variable. In contrast to a scatterplot, where all data points are plotted, in a line plot, only one point is plotted for every unique x-value or bin of x-values (like a histogram). If there are multiple observations in an x-bin, then the y-value of the point plotted in the line plot will be a summary statistic (like mean or median) of the data in the bin. The plotted points are connected with a line that emphasizes the sequential or connected nature of the x-values.\n",
    "\n",
    "If the x-variable represents time, then a line plot of the data is frequently known as a time series plot. Often, we have only one observation per time period, like in stock or currency charts. While there is a seaborn function tsplot that is intended to be used with time series data, it is fairly specialized and (as of this writing's seaborn 0.8) is slated for major changes.\n",
    "\n",
    "Instead, we will make use of Matplotlib's errorbar function, performing some processing on the data in order to get it into its necessary form.\n",
    "```python\n",
    "plt.errorbar(data = df, x = 'num_var1', y = 'num_var2')\n",
    "```\n",
    "<img src=https://video.udacity-data.com/topher/2018/March/5ab445d9_l4-c13-lineplot1/l4-c13-lineplot1.png>\n",
    "\n",
    "If we just blindly stick a dataframe into the function without considering its structure, we might end up with a mess like the above. The function just plots all the data points as a line, connecting values from the first row of the dataframe to the last row. In order to create the line plot as intended, we need to do additional work to summarize the data.\n",
    "```python\n",
    "# set bin edges, compute centers\n",
    "bin_size = 0.25\n",
    "xbin_edges = np.arange(0.5, df['num_var1'].max()+bin_size, bin_size)\n",
    "xbin_centers = (xbin_edges + bin_size/2)[:-1]\n",
    "\n",
    "# compute statistics in each bin\n",
    "data_xbins = pd.cut(df['num_var1'], xbin_edges, right = False, include_lowest = True)\n",
    "y_means = df['num_var2'].groupby(data_xbins).mean()\n",
    "y_sems = df['num_var2'].groupby(data_xbins).sem()\n",
    "\n",
    "# plot the summarized data\n",
    "plt.errorbar(x = xbin_centers, y = y_means, yerr = y_sems)\n",
    "plt.xlabel('num_var1')\n",
    "plt.ylabel('num_var2')\n",
    "```\n",
    "Since the x-variable ('num_var1') is continuous, we first set a number of bins into which the data will be grouped. In addition to the usual edges, the center of each bin is also computed for later plotting. For the points in each bin, we compute the mean and standard error of the mean. Note that the cut function call is simpler here than in the previous page, since we don't need to compute individual point weights.\n",
    "\n",
    "<img src=https://video.udacity-data.com/topher/2018/November/5becb159_l4-c13-lineplot2/l4-c13-lineplot2.png>\n",
    "\n",
    "An interesting part of the above summarization of the data is that the uncertainty in the mean generally increases with increasing x-values. But for the largest two points, there are no error bars. Looking at the default errorbar plot (or the scatterplot below), we can see this is due to there only being one point in each of the last two bins.\n",
    "Alternate Variations\n",
    "\n",
    "Instead of computing summary statistics on fixed bins, you can also make computations on a rolling window through use of pandas' rolling method. Since the rolling window will make computations on sequential rows of the dataframe, we should use sort_values to put the x-values in ascending order first.\n",
    "```python\n",
    "# compute statistics in a rolling window\n",
    "df_window = df.sort_values('num_var1').rolling(15)\n",
    "x_winmean = df_window.mean()['num_var1']\n",
    "y_median = df_window.median()['num_var2']\n",
    "y_q1 = df_window.quantile(.25)['num_var2']\n",
    "y_q3 = df_window.quantile(.75)['num_var2']\n",
    "\n",
    "# plot the summarized data\n",
    "base_color = sb.color_palette()[0]\n",
    "line_color = sb.color_palette('dark')[0]\n",
    "plt.scatter(data = df, x = 'num_var1', y = 'num_var2')\n",
    "plt.errorbar(x = x_winmean, y = y_median, c = line_color)\n",
    "plt.errorbar(x = x_winmean, y = y_q1, c = line_color, linestyle = '--')\n",
    "plt.errorbar(x = x_winmean, y = y_q3, c = line_color, linestyle = '--')\n",
    "\n",
    "plt.xlabel('num_var1')\n",
    "plt.ylabel('num_var2')\n",
    "```\n",
    "Note that we're also not limited to just one line when plotting. When multiple Matplotlib functions are called one after the other, all of them will be plotted on the same axes. Instead of plotting the mean and error bars, we will plot the three central quartiles, laid on top of the scatterplot.\n",
    "\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5becb168_l4-c13-lineplot3/l4-c13-lineplot3.png>\n",
    "\n",
    "\n",
    "Another bivariate application of line plots is to plot the distribution of a numeric variable for different levels of a categorical variable. This is another alternative to using violin plots, box plots, and faceted histograms. With the line plot, one line is plotted for each category level, like overlapping the histograms on top of one another. This can be accomplished through multiple errorbar calls using the methods above, or by performing multiple hist calls, setting the \"histtype = step\" parameter so that the bars are depicted as unfilled lines.\n",
    "```python\n",
    "bin_edges = np.arange(-3, df['num_var'].max()+1/3, 1/3)\n",
    "g = sb.FacetGrid(data = df, hue = 'cat_var', size = 5)\n",
    "g.map(plt.hist, \"num_var\", bins = bin_edges, histtype = 'step')\n",
    "g.add_legend()\n",
    "```\n",
    "Note that I'm performing the multiple hist calls through the use of FacetGrid, setting the categorical variable on the \"hue\" parameter rather than the \"col\" parameter. You'll see more of this parameter of FacetGrid in the next lesson. I've also added an add_legend method call so that we can identify which level is associated with each curve.\n",
    "<img src =https://video.udacity-data.com/topher/2018/March/5ab4b468_l4-c13-lineplot4/l4-c13-lineplot4.png>\n",
    "\n",
    "Unfortunately, the \"Alpha\" curve seems to be pretty lost behind the other three curves since the relatively low number of counts is causing a lot of overlap. Perhaps connecting the centers of the bars with a line, like what was seen in the first errorbar example, would be better.\n",
    "\n",
    "Functions you provide to the map method of FacetGrid objects do not need to be built-ins. Below, I've written a function to perform the summarization operations seen above to plot an errorbar line for each level of the categorical variable, then fed that function (freq_poly) to map.\n",
    "```python\n",
    "def freq_poly(x, bins = 10, **kwargs):\n",
    "    \"\"\" Custom frequency polygon / line plot code. \"\"\"\n",
    "    # set bin edges if none or int specified\n",
    "    if type(bins) == int:\n",
    "        bins = np.linspace(x.min(), x.max(), bins+1)\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2\n",
    "\n",
    "    # compute counts\n",
    "    data_bins = pd.cut(x, bins, right = False,\n",
    "                       include_lowest = True)\n",
    "    counts = x.groupby(data_bins).count()\n",
    "\n",
    "    # create plot\n",
    "    plt.errorbar(x = bin_centers, y = counts, **kwargs)\n",
    "\n",
    "bin_edges = np.arange(-3, df['num_var'].max()+1/3, 1/3)\n",
    "g = sb.FacetGrid(data = df, hue = 'cat_var', size = 5)\n",
    "g.map(freq_poly, \"num_var\", bins = bin_edges)\n",
    "g.add_legend()\n",
    "```\n",
    "**kwargs is used to allow additional keyword arguments to be set for the errorbar function.**\n",
    "\n",
    "(Documentation: numpy linspace)\n",
    "\n",
    "<img src = https://video.udacity-data.com/topher/2018/March/5ab545af_l4-c13-lineplot5/l4-c13-lineplot5.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Q plots ( Quantile - Quantile plots)\n",
    "\n",
    "There might be cases where you are interested to see how closely your numeric data follows some hypothetical distribution. This might be important for certain parametric statistical tests, like checking for assumptions of normality. In cases like this, you can use a quantile-quantile plot, or Q-Q plot, to make a visual comparison between your data and your reference distribution. Take for example the following comparison of the following data and a hypothetical normal distribution using the sample statistics:\n",
    "```python\n",
    "# create a histogram of the data\n",
    "bin_size = 0.5\n",
    "bin_edges = np.arange(4, 18 + bin_size, bin_size)\n",
    "plt.hist(data = df, x = 'num_var', bins = bin_edges);\n",
    "\n",
    "# overlay a theoretical normal distribution on top\n",
    "samp_mean = df['num_var'].mean()\n",
    "samp_sd = df['num_var'].std()\n",
    "\n",
    "from scipy.stats import norm\n",
    "x = np.linspace(4, 18, 200)\n",
    "y = norm.pdf(x, samp_mean, samp_sd) # normal distribution heights\n",
    "y *= df.shape[0] * bin_size # scale the distribution height\n",
    "\n",
    "plt.plot(x, y)\n",
    "```\n",
    "The matplotlib plot function is a generic function for plotting y-values against x-values, by default a line connecting each x-y pair in sequence. In this case, I first use numpy's linspace function to generate x-values across the range of the plot. Note that the first two arguments match the bin_edges limits, while the third argument specifies the number of values to generate between the two endpoints. Then, I use the scipy package's norm class to get the height of the normal distribution curve at those x-values, using the sample mean and standard deviation as distribution parameters. pdf stands for probability density function, which returns the normal distribution height (density) at each value of x. These values are such that the total area under the curve will add up to 1. Since we've got a histogram with absolute counts on the y-axis, we need to scale the curve so it's on the same scale as the main plot: we do this by multiplying the curve heights by the number of data points and bin size. The code above gives us the following plot:\n",
    "\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5be4d7a4_l4-c16-qqplot1/l4-c16-qqplot1.png>\n",
    "\n",
    "From a visual inspection of this overlaid plot, it looks like the data is a bit sparse on the right side compared to the expected normal distribution. There's also a bit of a spike of values between 11 and 12. On the other hand, the left side of the curve isn't too far off from the expected distribution, though it might be said that we might be missing some expected points in the left tail of the distribution. The question that we'd like to address is if there's enough evidence from what we've observed to say that the data is significantly different from the expected normal distribution.\n",
    "\n",
    "One way we could approach this is through a statistical test, such as using scipy's shapiro function to perform the Shapiro-Wilk test. But since this is a course on data visualization, we'll inspect this question visually, using the Q-Q plot type teased at the top of the page. The main idea of the plot is this: if the data was normally distributed, then we'd expect a certain pattern in terms of how far each data point is from the mean of the distribution. If we order the points from smallest to largest, then we could compare how large the _k_-th ranked data point is against the _k_-th ranked point from the expected distribution.\n",
    "\n",
    "To get these expected values, we'll make use of the norm class's ppf function, which stands for percent point function. The ppf takes as input a proportion (valued between 0 and 1) and returns the value in the distribution that would leave that proportion of the curve to the left. For a standard normal distribution (mean = 0, standard deviation = 1), the ppf(0.25)=−0.674, ppf(0.25) = -0.674, ppf(0.25)=−0.674, ppf(0.5)=0, ppf(0.5) = 0, ppf(0.5)=0, and ppf(0.75)=0.674, ppf(0.75) = 0.674, ppf(0.75)=0.674. The main question, then, is what values to stick into the ppf.\n",
    "\n",
    "There's a few different conventions around this, but they generally take the form of the following equation:\n",
    "\n",
    "   Given _n_ data points, the _k_-th value should be at probability point k−an+1−2a\\frac{k-a}{n+1-2a}n+1−2ak−a​, for some _a_ between 0 and 1 (inclusive).\n",
    "\n",
    "This equation distributes the probability points symmetrically about 0.5, and adjusting _a_ changes how much probability is left in the tails of the [0,1] range. Commonly, _a_ is set to a balanced value of 0.5, which gives the equation k−0.5n\\frac{k-0.5}{n}nk−0.5​. Let's put this all together using code:\n",
    "```python\n",
    "n_points = df.shape[0]\n",
    "qs = (np.arange(n_points) - .5) / n_points\n",
    "expected_vals = norm.ppf(qs, samp_mean, samp_sd)\n",
    "\n",
    "plt.scatter(expected_vals, df['num_var'].sort_values())\n",
    "plt.plot([4,18],[4,18],'--', color = 'black')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Expected Values')\n",
    "plt.ylabel('Observed Values')\n",
    "```\n",
    "It's a good idea to label the axes in this case. Since the actual and expected data are both on the same scale, the labels are a big help to keep things clear. In addition, rather than just plotting the expected and actual data alone, I've also added another plot call to add a diagonal x = y line. If the data matches the actual values perfectly on the expected value, they will fall directly on that diagonal line. The plt.axis('equal') line supports the visualization, as it will set the axis scaling to be equal, and the diagonal line will be at a 45 degree angle.\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5be5e565_l4-c16-qqplot2/l4-c16-qqplot2.png>\n",
    "\n",
    "Excepting the smallest and largest few points, the distribution of observed values is actually fairly in line with the distribution of expected values – that is, it falls along the diagonal line. The smallest and largest observed points are larger than the values that would be expected from the normal distribution, but it's not by much. Given how much farther values can get spread out in the tails of the normal distribution, this shouldn't be a major concern. We're probably fine in treating the data as normally distributed.\n",
    "\n",
    "Usually, the Q-Q plot is computed and rendered in terms of standardized units, rather than the scale of the original data. A standardized dataset has a mean of 0 and standard deviation of 1, so to convert a set of values into standard scores, we just need to subtract the sample mean from each value to center it around 0, then divide by the sample standard deviation to scale it. Calling methods of the norm class without arguments for the mean or standard deviation assume the standard normal distribution. The code changes as follows:\n",
    "```python\n",
    "expected_scores = norm.ppf(qs)\n",
    "data_scores = (df['num_var'].sort_values() - samp_mean) / samp_sd\n",
    "\n",
    "plt.scatter(expected_scores, data_scores)\n",
    "plt.plot([-2.5,3],[-2.5,3],'--', color = 'black')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Expected Standard Scores')\n",
    "plt.ylabel('Observed Standard Scores')\n",
    "```\n",
    "\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5be600cf_l4-c16-qqplot3/l4-c16-qqplot3.png>\n",
    "Notice that the shape of the data has not changed since both datasets have been scaled in the exact same way. One of the reasons for performing this scaling is that it makes it easier to talk about the data values against the expected, theoretical distribution. In the first plot, there's no clear indication of where the center of the data lies, and how spread out the data is from that center. In the latter plot, we can use our expectations for how much of the data should be one or two standard deviations from the mean to better understand how the data is distributed. It also separates the values of the theoretical distribution from any properties of the observed data.\n",
    "\n",
    "Before closing this page out, let's take a quick look at the Q-Q plot when the data distribution does not fit the normal distribution assumptions. Instead of generating data from a normal distribution, I'll now generate data from a uniform distribution:\n",
    "```python\n",
    "# generate the data\n",
    "np.random.seed(8322489)\n",
    "\n",
    "n_points = 120\n",
    "unif_data = np.random.uniform(0, 10, n_points)\n",
    "\n",
    "# set up the figure\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left subplot: plot the data\n",
    "plt.subplot(1, 2, 1)\n",
    "bin_size = 0.5\n",
    "bin_edges = np.arange(0, 10 + bin_size, bin_size)\n",
    "plt.hist(x = unif_data, bins = bin_edges);\n",
    "\n",
    "# overlay a theoretical normal distribution on top\n",
    "samp_mean = unif_data.mean()\n",
    "samp_sd = unif_data.std()\n",
    "\n",
    "from scipy.stats import norm\n",
    "x = np.linspace(0, 10, 200)\n",
    "y = norm.pdf(x, samp_mean, samp_sd) # normal distribution heights\n",
    "y *= n_points * bin_size # scale the distribution height\n",
    "plt.plot(x, y)\n",
    "\n",
    "# right subplot: create a Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "qs = (np.arange(n_points) - .5) / n_points\n",
    "expected_scores = norm.ppf(qs)\n",
    "data_scores = (np.sort(unif_data) - samp_mean) / samp_sd\n",
    "\n",
    "plt.scatter(expected_scores, data_scores)\n",
    "plt.plot([-2.5,2.5],[-2.5,2.5],'--', color = 'black')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Expected Standard Scores')\n",
    "plt.ylabel('Observed Standard Scores')\n",
    "```\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5be602af_l4-c16-qqplot4/l4-c16-qqplot4.png>\n",
    "Left: Original data; Right: Q-Q plot\n",
    "\n",
    "When we compare the random standardized scores drawn from the uniform distribution to the expected scores from the theoretical normal distribution in the Q-Q plot, we see an S-shaped curve. The comparison of values in the middle of the curve are approximately linear in trend, but the slope is steeper than the desired y = x. Meanwhile on the edges, the slope is extremely shallow, as the uniform distribution is fixed to a finite range, but the normal distribution values in the tails are expected to be much further away. You can somewhat see this in the superimposed distribution line in the left-side plot, where even at the edges of the data, there is still quite a bit of height to the theoretical normal curve. All of this contributes to the result that the randomly-generated uniform data can't be well-approximated by the normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Plots\n",
    "\n",
    "One common plotting technique has not been discussed thus far in the course, and that’s stacking. Stacked bar charts and histograms are not uncommon, but there are often better plot choices available.\n",
    "\n",
    "The most basic stacked chart takes a single bar representing the full count, and divides it into colored segments based on frequencies on a categorical variable. If this sounds familiar, that's because it almost perfectly coincides with the description of a pie chart, except that the shape being divided is different.\n",
    "```python\n",
    "# pre-processing: count and sort by the number of instances of each category\n",
    "sorted_counts = df['cat_var'].value_counts()\n",
    "\n",
    "# establish the Figure\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left plot: pie chart\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
    "        counterclock = False);\n",
    "plt.axis('square');\n",
    "\n",
    "# right plot: horizontally stacked bar\n",
    "plt.subplot(1, 2, 2)\n",
    "baseline = 0\n",
    "for i in range(sorted_counts.shape[0]):\n",
    "    plt.barh(y = 1, width = sorted_counts[i], left = baseline)\n",
    "    baseline += sorted_counts[i]\n",
    "\n",
    "plt.legend(sorted_counts.index)  # add a legend for labeling\n",
    "plt.ylim([0,2]) # give some vertical spacing around the bar\n",
    "```\n",
    "\n",
    "\n",
    "The stacked bar is built through successive calls of the matplotlib barh function; each time the function is called, the bar that is plotted is assigned a new color. The choice of \"y\" is arbitrary: it'll just center the bar around y = 1, but it doesn't have any inherent meaning. The \"left\" parameter specifies the left edge of each bar added to the stack, which starts at the baseline of 0 and is built up with each stacked bar. Note in this case that the bar is being plotted with absolute counts, rather than proportions. A discussion of absolute vs. relative frequencies will come later down the page!\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5bdcb28b_l4-c19-stackedbars1/l4-c19-stackedbars1.png>\n",
    "\n",
    "Given this similarity, cautions regarding use of the stacked bar are fairly similar to that of the pie chart:\n",
    "- Make sure that relative frequencies are a meaningful comparison.\n",
    "- Try to limit yourself to a small number of categories, up to about five.\n",
    "- Make sure that categories are arranged in a sensible order, e.g. by frequency for nominal data or by levels for ordinal data.\n",
    "\n",
    "Otherwise, the standard bar chart is a reliable option that should be used in most cases. Only use the pie chart or singly divided bar if there's a compelling reason to do so.\n",
    "\n",
    "The debate becomes more interesting when multiple features get involved. When should we feel free to create a stacked bar chart versus using a clustered bar chart? There are two major categories of stacked bar chart that I want to focus on here: plotting by absolute frequency and plotting by relative frequency. We'll start with code for an absolute frequency stacked chart below.\n",
    "```python\n",
    "cat1_order = ['East', 'South', 'West', 'North']\n",
    "cat2_order = ['Type X', 'Type Y', 'Type Z', 'Type O']\n",
    "\n",
    "plt.figure(figsize = [12, 5])\n",
    "\n",
    "# left plot: clustered bar chart, absolute counts\n",
    "plt.subplot(1, 2, 1)\n",
    "sb.countplot(data = df, x = 'cat_var1', hue = 'cat_var2',\n",
    "             order = cat1_order, hue_order = cat2_order)\n",
    "plt.legend()\n",
    "\n",
    "# right plot: stacked bar chart, absolute counts\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "baselines = np.zeros(len(cat1_order))\n",
    "# for each second-variable category:\n",
    "for i in range(len(cat2_order)):\n",
    "    # isolate the counts of the first category,\n",
    "    cat2 = cat2_order[i]\n",
    "    inner_counts = df[df['cat_var2'] == cat2]['cat_var1'].value_counts()\n",
    "    # then plot those counts on top of the accumulated baseline\n",
    "    plt.bar(x = np.arange(len(cat1_order)), height = inner_counts[cat1_order],\n",
    "            bottom = baselines)\n",
    "    baselines += inner_counts[cat1_order]\n",
    "\n",
    "plt.xticks(np.arange(len(cat1_order)), cat1_order)\n",
    "plt.legend(cat2_order)\n",
    "```\n",
    "The strategy for this plot is very similar to the single stacked bar shown above, except that we're using the standard bar with \"x\" and \"bottom\" parameters, and that baselines is a list of base heights. We want to create all of the bars for a particular secondary category at the same time so that creation of the legend has a 1:1 mapping to bar calls. You might notice below that the order of labels in the legend is the reverse of the order in which the bars are stacked. You'll see code to handle this in the relative frequency plot below!\n",
    "\n",
    "<img src=https://video.udacity-data.com/topher/2018/November/5be095c0_l4-c19-stackedbars2/l4-c19-stackedbars2.png>\n",
    "\n",
    "The stacked bar chart plotted by absolute frequency carries one big advantage over the clustered bar chart: for the variable plotted on the x-axis, it's clear which category level has the highest frequency, in this case \"East\". The values of this variable can be interpreted just like the univariate bar chart. The disadvantage of the stacked bar chart comes with interpretation of the second, stacked variable. If you want to compare the relative counts of this second variable across levels of the first, you can really only do that for the category plotted on the baseline, which in this case is the blue one, \"Type X\". For the remaining categories, it's much harder to make the comparison of values – you can't really tell that the counts of \"Type O\" are larger in the \"South\" than the \"North\" from the stacked chart, where it's directly comparable in the clustered bar chart.\n",
    "\n",
    "Now, let's take a look at what happens when we create the stacked bar chart with relative frequencies instead, where each bar is scaled to a total height of 1.\n",
    "```python\n",
    "cat1_order = ['East', 'South', 'West', 'North']\n",
    "cat2_order = ['Type X', 'Type Y', 'Type Z', 'Type O']\n",
    "\n",
    "artists = [] # for storing references to plot elements\n",
    "baselines = np.zeros(len(cat1_order))\n",
    "cat1_counts = df['cat_var1'].value_counts()\n",
    "\n",
    "# for each second-variable category:\n",
    "for i in range(len(cat2_order)):\n",
    "    # isolate the counts of the first category,\n",
    "    cat2 = cat2_order[i]\n",
    "    inner_counts = df[df['cat_var2'] == cat2]['cat_var1'].value_counts()\n",
    "    inner_props = inner_counts / cat1_counts\n",
    "    # then plot those counts on top of the accumulated baseline\n",
    "    bars = plt.bar(x = np.arange(len(cat1_order)),\n",
    "                   height = inner_props[cat1_order],\n",
    "                   bottom = baselines)\n",
    "    artists.append(bars)\n",
    "    baselines += inner_props[cat1_order]\n",
    "\n",
    "plt.xticks(np.arange(len(cat1_order)), cat1_order)\n",
    "plt.legend(reversed(artists), reversed(cat2_order), framealpha = 1,\n",
    "           bbox_to_anchor = (1, 0.5), loc = 6);\n",
    "```\n",
    "There are two main changes to this code compared to the previous plot. First of all, the cat1_counts variable has been computed to change the absolute frequencies into relative frequencies within each x-axis category. Secondly, some code has been added to reverse the order of bars in the legend. The artists variable has been added to store references to each of the bar groups added from each bar call. Then in the legend function call, we make use of the built-in Python function reversed to reverse the order in which the artists and labels are included in the legend. The additional parameters affect the positioning of the legend: setting an anchor for the legend box on the right side of the plot via \"bbox_to_anchor\", and positioning the anchor to the legend's left with \"loc = 6\".\n",
    "\n",
    "<img src = https://video.udacity-data.com/topher/2018/November/5be0a0aa_l4-c19-stackedbars3/l4-c19-stackedbars3.png>\n",
    "\n",
    "Since the bars are all the same height of 1 with a relative frequency stacked bar chart, we lose the ability to compare the absolute counts on the categorical variable plotted on the x-axis (i.e. we can't tell that \"East\" has the most counts and \"North\" the least amount). In exchange, we can now compare the relative prevalence of the stacked variable on both the first category on the bottom (\"Type X\") as well as the category on the top (\"Type O\"). We can now see that, in terms of relative frequency, \"Type X\" has a fairly consistent presence in \"South\", \"West\", and \"North\", and that \"Type O\" has its highest relative frequency in \"North\". Unfortunately, this still doesn't help us make easy comparisons about the \"Type Y\" and \"Type Z\" categories that are sandwiched in between. This major limitation is a big reason why other plot types like clustered bar or line charts are often preferable to stacking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridgeline Plots\n",
    "\n",
    "One of the hot new visualization types from recent years is the ridgeline plot. In a nutshell, the ridgeline plot is a series of vertically faceted line plots or density curves, but with somewhat overlapping y-axes. This can be thought of as a contrast to the line plot variation seen in the \"Line Plots\" part of the lesson, where multiple lines were plotted on the same axes, with different hues. On this page, I'll walk through the creation of a ridgeline plot using some of the demonstration data shown in the \"Faceting\" page:\n",
    "```python\n",
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, col = 'many_cat_var', col_wrap = 5, size = 2,\n",
    "                 col_order = group_order)\n",
    "g.map(plt.hist, 'num_var', bins = np.arange(5, 15+1, 1))\n",
    "g.set_titles('{col_name}')\n",
    "```\n",
    "Faceted plot, whose data will be converted into a ridgeline form\n",
    "<img src=https://video.udacity-data.com/topher/2018/March/5ab2f284_l4-c11-faceting3/l4-c11-faceting3.png>\n",
    "\n",
    "Two things immediately come to mind for changing the faceted histograms into a ridgeline plot. First of all, changing the form of the distribution plots from histograms to kernel density estimates (as seen in the Extras of the previous lesson) will make the overlaps a bit smoother. Second, we need to facet the levels by rows so that they're all stacked up on top of one another.\n",
    "```python\n",
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, row = 'many_cat_var', size = 0.75, aspect = 7,\n",
    "                 row_order = group_order)\n",
    "g.map(sb.kdeplot, 'num_var', shade = True)\n",
    "g.set_titles('{row_name}')\n",
    "```\n",
    "FacetGrid and set_titles change \"col\" to \"row\", also removing \"col_wrap\". The \"size\" and \"aspect\" dimensions have also been adjusted for the large vertical stacking of facets. The map function changes to kdeplot and removes \"bins\", adding the \"shade\" parameter in its place.\n",
    "<img src=https://video.udacity-data.com/topher/2018/March/5ab2f284_l4-c11-faceting3/l4-c11-faceting3.png>\n",
    "\n",
    "Now we've got all of the group distributions stacked on top of one another for a uni-dimensional comparison, but the plot's still pretty tall. Next, we'll create some overlap between the individual subplots.\n",
    "```python\n",
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "# adjust the spacing of subplots with gridspec_kws\n",
    "g = sb.FacetGrid(data = df, row = 'many_cat_var', size = 0.5, aspect = 12,\n",
    "                 row_order = group_order, gridspec_kws = {'hspace' : -0.2})\n",
    "g.map(sb.kdeplot, 'num_var', shade = True)\n",
    "\n",
    "# remove the y-axes\n",
    "g.set(yticks=[])\n",
    "g.despine(left=True)\n",
    "\n",
    "g.set_titles('{row_name}')\n",
    "```\n",
    "I've added the \"gridspec_kws\" parameter to the FacetGrid call to adjust the arrangement of subplots in the grid through Matplotlib's GridSpec class. By setting \"hspec\" to a negative value, the subplot axes bounds will overlap vertically. The \"size\" and \"aspect\" parameters have also been adjusted. While I'm at it, I'll add some code on the FacetGrid object to remove the y-axis through the despine method and remove the ticks through the set method. They're going to start overlapping, and we don't really need them – we're mostly interested in the relative positions of the distributions rather than specific heights.\n",
    "\n",
    "<img src=https://video.udacity-data.com/topher/2018/November/5be0dd78_l4-c20-ridgeline2/l4-c20-ridgeline2.png>\n",
    "\n",
    "The individual subplots now overlap, but we've still got a problem: the backgrounds of the subplots are opaque, thus obscuring all but the tops of all of the individual group distributions, with the exception of the lowest. In addition, the individual subplot titles overlap the other distributions with some ambiguity: these should be moved elsewhere in the individual plots. The revised code and plot look like this:\n",
    "```\n",
    "group_means = df.groupby(['many_cat_var']).mean()\n",
    "group_order = group_means.sort_values(['num_var'], ascending = False).index\n",
    "\n",
    "g = sb.FacetGrid(data = df, row = 'many_cat_var', size = 0.5, aspect = 12,\n",
    "                 row_order = group_order, gridspec_kws = {'hspace' : -0.2})\n",
    "g.map(sb.kdeplot, 'num_var', shade = True)\n",
    "\n",
    "g.set(yticks=[])\n",
    "g.despine(left=True)\n",
    "\n",
    "# set the transparency of each subplot to full\n",
    "g.map(lambda **kwargs: plt.gca().patch.set_alpha(0))\n",
    "\n",
    "# remove subplot titles and write in new labels\n",
    "def label_text(x, **kwargs):\n",
    "    plt.text(4, 0.02, x.iloc[0], ha = 'center', va = 'bottom')\n",
    "g.map(label_text, 'many_cat_var')\n",
    "g.set_xlabels('num_var')\n",
    "g.set_titles('')\n",
    "```\n",
    "We make clever use of the FacetGrid object's map function to perform the plot modifications. Previously, you've seen map used where the first argument is a plotting function, the following arguments are positional variable strings, and any additional arguments are keyword arguments for the plotting function. In actuality, you can set any function as the first argument, which will be applied to each facet. To apply the transparency using map, I set up an anonymous lambda function that gets the current Axes (gca), selects its background (patch), and sets its transparency to full.\n",
    "\n",
    "As for the second map argument, it sends a pandas Series to the function specified by the first argument. This Series is filtered to include only the column specified by the second map argument, with only the rows appropriate for each facet. In this case, I exploit the fact that the 'many_cat_var' column is filled with copies of the categorical feature string to specify the text string, with hardcoded positional values appropriate to the plot. (map also sends a few general keyword arguments like 'color' automatically to the specified function, hence the need for **kwargs to capture them despite not specifying any myself.) One downside to this approach is that the x-axis labels get replaced with 'many_cat_var' after the map call, thus requiring the addition of a set_xlabels function call to reset the string.\n",
    "\n",
    "The final ridgeline plot looks like this, where we can see the distribution of our numeric variable on each category, sorted by mean:\n",
    "<img src=https://video.udacity-data.com/topher/2018/November/5be0ea2b_l4-c20-ridgeline3/l4-c20-ridgeline3.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
